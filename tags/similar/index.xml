<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>â€“ similar</title><link>/tags/similar/</link><description>Recent content in similar on</description><generator>Hugo -- gohugo.io</generator><atom:link href="/tags/similar/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Filter by Page Similarity</title><link>/docs/examples/filter-similar/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/examples/filter-similar/</guid><description>
&lt;h2 id="filter-response-by-similarity-to-a-given-page">Filter Response by Similarity to A Given Page&lt;/h2>
&lt;div class="alert alert-warning" role="alert">
&lt;h4 class="alert-heading">Heads up&lt;/h4>
&lt;ul>
&lt;li>SSDeep/&lt;code>--filter-similar-to&lt;/code> does not do well at detecting similarity of very small responses
&lt;ul>
&lt;li>The lack of accuracy with very small responses is considered a fair trade-off for not negatively impacting performance&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Using a bunch of &lt;code>--filter-similar-to&lt;/code> values &lt;strong>may&lt;/strong> negatively impact performance&lt;/li>
&lt;/ul>
&lt;/div>
&lt;p>Version 1.11.0 adds the ability to specify an example page for filtering pages that are similar to the given example.&lt;/p>
&lt;p>For example, consider a site that attempts to redirect new users to a &lt;code>/register&lt;/code> endpoint. The &lt;code>/register&lt;/code> page has a
CSRF token that alters the page&amp;rsquo;s response slightly with each new request (sometimes affecting overall length). This
means that a simple line/word/char filter won&amp;rsquo;t be able to filter all responses. In order to filter those redirects out,
one could use a command like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>./feroxbuster -u https://somesite.xyz --filter-similar-to https://somesite.xyz/register
&lt;/code>&lt;/pre>&lt;p>&lt;code>--filter-similar-to&lt;/code> requests the page passed to it via CLI (&lt;code>https://somesite.xyz/register&lt;/code>), after which it hashes
the response body using the &lt;a href="https://ssdeep-project.github.io/ssdeep/index.html">SSDeep algorithm&lt;/a>. All subsequent
pages are hashed and compared to the original request&amp;rsquo;s hash. If the comparison of the two hashes meets a certain
percentage of similarity (currently 95%), then that request will be filtered out.&lt;/p>
&lt;p>SSDeep was selected as it does a good job of identifying near-duplicate pages once content-length reaches a certain
size, while remaining performant. Other algorithms were tested but resulted in huge performance hits (orders of
magnitude slower on requests/second).&lt;/p></description></item></channel></rss>